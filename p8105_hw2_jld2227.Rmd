---
title: "HW2 p8105 James Dalgleish jld2227"
author: "James Dalgleish"
date: "September 27, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
#### Problem 1
Instructions(I'm choosing to include these to make it a bit easier to grade):
"Read and clean the data; retain line, station, name, station latitude / 
longitude, routes served, entry, vending, entrance type, and ADA compliance.
Convert the entry variable from character (YES vs NO) to a logical variable
(the ifelse or recode function may be useful)."
```{r data_import}
subway_data = read_csv("./problem1/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% 
  select(c("line", "station_name", "station_latitude", "station_longitude",
           starts_with("route"), "entry", "vending", "ada")) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
subway_data 
```
###### Data description
Instruction: "Write a short paragraph about this dataset – 
explain briefly what variables the dataset contains, 
describe your data cleaning steps so far,
and give the dimension (rows x columns) of the resulting dataset.
Are these data tidy?"

Essentially, the dataset contains two character variables dening the subway line and station,
two numeric (double) variables denoting geolocation (latitude and longitude),
11 variables denoting the routes (mostly populated with missing values) in a wide format,
two logical TRUE/FALSE variables for entry and ada,
and a character variable with values "YES" and "NO" variable for vending.
The data is not tidy for at least the reasons that routes are in a wide format 
where the route is encoded in the column. This can be remedied with the 
gather function. As we'll see below, route1 was been encoded in
mixed case(see the "E" and "e"?),
which may mess up some downstream analysis as well.

The dataset contains `r nrow(subway_data)` rows and `r ncol(subway_data)`
columns. Data cleaning included reading the data into a tibble, 
selecting the necessary columns, converting the column names 
into a standard format, and converting the entry variable into a logical column.
```{r describe}
skimr::skim( subway_data )
subway_data %>% 
  select(starts_with( "route" )) %>%
  sapply(., table)
```
 Putting the wide format into long could aid analysis and can be done with a 
 simple gather command. We'll also get rid of names that appear to be the same, 
 but differ only on case. There are stations with FS and GS lines, but
which might seem like more than one line in a single observation, but they are
distinct lines if one looks an MTA licensed site:
https://moveonmap.com/nyc/lnG_GS_FS/
If they were F and S incorrectly noted in the same row, we could consider 
using separate_rows() to fix that.
We'll use the table function to show that we've now fixed the lower
and upper case e.

```{r wide_to_long}
subway_data_long = subway_data %>% 
  #others may call this route name rather than train.
  #I simply used a different nomenclature, but the logic is the same.
  gather(key = "route_number", value = "train", route1:route11) %>% 
  mutate(train = tolower(train)) #This fixes the lower case e.
subway_data_long #display the dataframe.
subway_data_long %>% 
  pull(train) %>% 
  table()
```
####### Distinct stations
Instruction: "Answer the following questions using these data:

How many distinct stations are there? Note that stations are identified both by
name and by line (e.g. 125th St A/B/C/D; 125st 1; 125st 4/5); 
the distinct function may be useful here. How many stations are ADA compliant?
What proportion of station entrances / exits without vending allow entrance?"

We'll now select the columns of interest and limit to only the distinct rows,
then count the number of distinct rows containing route number and train
combinations.
```{r distinct_stations}
subway_routes_trains <- subway_data_long %>%
  select("route_number", "train") %>%
  distinct() %>% 
  na.omit() %>% 
  arrange(route_number,train)
subway_routes_trains
nrow(subway_routes_trains)
```
There happen to be `r nrow(subway_routes_trains)` distinct stations,
according to the data.

####### Ada compliance
We will create an additional variable denoting if the station ada is compliant.
There can be multiple entrances, only some of which are ada compliant.
As a simple rule, a station is compliant if there is at least one station 
name/line combination with an ada compliant entrance, then it is ada compliant.
```{r n_ada_compliant_stations}
#data is read from the dataframe in long format.
ada_accessible_stations <- subway_data_long %>% 
  #Colums are selected from the dataframe.
  select("route_number", "train", "ada") %>% 
  #Those openings that are accessible are  filtered.
  filter(ada == TRUE) %>%
  #Those that have a distinct route number and name (train) are kept.
  distinct(route_number, train, .keep_all = TRUE) %>% 
  #Rows with missing values are omitted.
  na.omit %>% 
  #The dataframe is sorted by route number and train.
  arrange(route_number, train)
ada_accessible_stations
```

####### Station entrances/exists that without vending which allow entry.
Instruction: "What proportion of station entrances /
exits without vending allow entrance?"
The original form "contains information related to each entrance and exit for
each subway station in NYC."

So, we will use this format to answer a question about entrances.
Careful verification through subsetting for duplicate locations reveals that 
this is the case.
Even looking at the latitude and longitude of the entrance does not distinctly 
identify the station.
In some cases, there is a different kind of entrance (elevator/escalator) or
the station is located on a different
corner of an intersection.
Converting wide to long as before could mislead someone to thinking that
there are more entrances than exist in cases where the entrance services 
multiple routes.
Therefore, the way to solve this problem is simple...
read the original data in with minimal processing, filter on the vending and
entry criteria, then count the number of rows. Using the distinct function
can prove again, that the rows are distinct.
```{r ent_exit_wo_vending}
subway_ent_exit_locations = read_csv(
  "./problem1/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() #read data and clean names
#The below gets the distinct entry and exit locations.
subway_ent_exit_locations %>% 
  distinct()  %>% 
  nrow()
#the below gets the total number of entry and exit locations.
subway_ent_exit_locations %>% 
  nrow()
#duplicated locations are listed below (alluded to in paragraph answer).
duplicated_locations<-subway_ent_exit_locations %>%
  filter(duplicated(entrance_location)) %>% 
  pull(entrance_location)
#duplicated stations are shown in this dataframe (mentioned in paragraph
#above).
duplicated_locations_subset<-subway_ent_exit_locations %>% 
  filter(entrance_location %in% duplicated_locations) %>% 
  arrange()
#filter the subway openings for enterable locations with vending.
enterable_novend <- subway_ent_exit_locations %>% 
  filter(.,vending == "NO" & entry == "YES") 
enterable_novend %>% 
  nrow() 
#count the number of enterable locations with vending.
n_enterable_novend <- enterable_novend %>% 
  distinct() %>% 
  nrow() 
#divide the count of the enterable locations with vending by the total number of openings
enterable_novend_prop =  n_enterable_novend / nrow(subway_ent_exit_locations)
#get the number of openings without vending.
n_novend <- subway_ent_exit_locations %>% 
  filter(.,vending == "NO") %>% 
  nrow()
#get the proportion of stations that are enterable given there is no vending.
prop_enterable_of_novend = n_enterable_novend / n_novend
  
```

The number of distinct enterable subway entrances/exits without vending is
therefore `r n_enterable_novend`. The proportion of
such stations out of the total is `r enterable_novend_prop`.
If one wants the proportion of openings that are enterable of
all the openings without vending (another way of looking at this question):
`r prop_enterable_of_novend`.


Instruction: "Reformat data so that route number and route name
are distinct variables. 
How many distinct stations serve the A train? How many are ADA compliant?"

First we'll tackle the number of distinct A train stations.
We've done most of the work by using gather() earlier.
The problem description's route name I have chosen to call train
(the value argument in gather).
I will change the name to make this clearer for the reader (from "train" to 
"route name").
```{r distinct_a_stations}
subway_data_long_a = subway_data %>% 
  gather(key = "route_number",
  value = "route_name",route1:route11) %>% 
  mutate(line = tolower(line))   %>% 
  filter(route_name == "A") %>% 
  distinct(station_name,line) %>%
  arrange(station_name,line)
subway_data_long_a
```

It becomes clear now that there are precicsely `r nrow(subway_data_long_a)`
stations that service the A train.

Finally, how many are ada compliant follows a nearly identical process using the 
filter function.
```{r n_ada_stations}
subway_data_long_ada = subway_data %>% 
  gather(key = "route_number", value = "route_name", route1:route11) %>% 
  filter(ada == TRUE) %>% 
  mutate(line = tolower(line))   %>% 
  distinct(station_name, line) %>%
  arrange(station_name, line)
subway_data_long_ada
```

It becomes clear now that there are precicsely `r nrow(subway_data_long_ada)`
stations where there is an accessible entrance. Assuming one entrance is
sufficent for compliance per station, then this represents the number of
accessible stations.

#### Problem 2
instruction: "Read and clean the Mr. Trash Wheel sheet:

*specify the sheet in the Excel file and to omit columns containing notes (using the range argument and cell_cols() function)
*use reasonable variable names
*omit rows that do not include dumpster-specific data
*rounds the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)"

"Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful)."

I've chosen to read in the data, specify the sheet, columns, drop the total rows
(which typically have NA in dumpster), and removed any row without a month that
matches base-r's month names constants. This means that if the month isn't one of 
"January", "February", or any of the other ones, the row is removed.
One line has an incorrect year (the 1/2/2017 entry). This, along with the empty
yeared rows near the end of the table, as indicated by missmap, will be corrected by reassigning the year.
lubridate::year() can be used instead of format() and as.integer(),
I've chosen to use missmap to show that I've fixed the missing values correctly.
read_excel doesn't convert to a tibble, so I've chosen to do that for purposes of display.

```{r import_tidy_mr_trash_wheel}
trash_wheel_data <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
  sheet =  "Mr. Trash Wheel",
  range = readxl::cell_cols("A:N")) %>% #This limits to columns A:N,
  #which are the data columns
  janitor::clean_names() %>%  #converts colnames to snake case.
  drop_na(dumpster) %>% #removes all NA in dumpster
  filter( month %in% month.name) %>%  #filters those without a valid month.
  mutate( year = lubridate::year(date), #fixes the year variable to pull from the
          #date-- some of the year associated with the date were inaccurate.
    sports_balls = round(sports_balls) %>% #grabs the sports balls and rounds
            as.integer()) %>%  #converts rounded values from double to integer.
  as.tibble() #convert to tibble.
 trash_wheel_data #display the data.
```
Instruction:  "Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful)."
An import of both sheets followed by a merge is done below. We read the excel file,
specify the 2017 sheet, then instruct the reader to skip the
first two rows that don't contain meaningful rows. Following this, the column
names are specified.
```{r import_precip}
#I was instructed by a TA to put in some inline comments, for ease of grading.
precip_2017 <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
    #This reads the excel sheet.
  sheet =  "2017 Precipitation", #This specifies sheet
  skip = 2, col_names = c("month", "tot_precip")) %>% 
  #skip rows w/o data, limit to a pair of columns
  na.omit() %>%  #get rid of any rows containing empty cells.
  as.tibble() %>% #This converts to tibble format
  mutate(year = 2017) #Here, we add a year variable.
#We follow the same steps for the 2016 dataset
precip_2016 <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
  sheet =  "2016 Precipitation", 
  skip = 2,
  col_names = c("month", "tot_precip")) %>% 
  na.omit() %>% 
  as.tibble() %>% 
  mutate(year = 2016)
#Below we combine datasets vertically.
precip_2017_2016 <- dplyr::bind_rows(precip_2016, precip_2017) %>% 
  mutate(month = month.name[month])
```
Below, we'll calculate the paragraph statistics, starting by getting the total
weight (in tons) per year and taking the mean of that by year sum. We follow a 
similar process to calculate the sd and mean of the homes powered each year, but
filter the 2014 year out as no measures were made.
For the precipitation data, the column is pulled from the tibble and summed
to a single number, while the median number.
```{r calculate_paragraph_stats}
avg_wt_removed_yr <- trash_wheel_data %>% 
  group_by(year) %>% #This converts the df to a df grouped by year.
  summarise(sum = sum(weight_tons)) %>% #gets the yearly tonnage
  pull(sum) %>%  #pulls out the tonnage sums by year as a vector.
  mean() #calculates the mean on the yearly weight sums.
homes_yr_sd <- trash_wheel_data %>%
  group_by(year) %>%
  summarise(sum = sum(homes_powered)) %>%
  filter(year != 2014) %>% 
  pull(sum) %>%  sd()
homes_yr_mean <- trash_wheel_data %>%
  group_by(year) %>%
  summarise(sum = sum(homes_powered)) %>%
  filter(year != 2014) %>% 
  pull(sum) %>%  mean()
prcp_17_tot <- precip_2017 %>% 
  pull(tot_precip) %>%
  sum()
med_sports_balls <- trash_wheel_data %>% 
  filter(year == 2016) %>% 
  pull(sports_balls) %>% 
  median()
```
Instruction:"Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables."

The number of observations in the trash wheel data was `r nrow(trash_wheel_data)` and `r nrow(precip_2017_2016)` observations in the precipitation data. The key variables might be the more reportable effects the intervention had on the environment, namely the average trash removed per year , `r avg_wt_removed_yr` tons, or the postitive outcomes outside of trash removal, namely the average homes powered per year, `r homes_yr_mean` homes along with it's minimal standard deviation `r homes_yr_sd` homes during years where this variable has a measured value (not 2014).  

Instruction: " For available data, what was the total precipitation in 2017? What was the median number of sports balls in a dumpster in 2016?"
The total precipitation in 2017 was `r prcp_17_tot` inches.
The median number of sports balls was `r med_sports_balls`.




#### Problem 3

"This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package.

For this question:

format the data to use appropriate variable names;
focus on the “Overall Health” topic
exclude variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation
structure data so that responses (excellent to poor) are variables taking the value of Data_value
create a new variable showing the proportion of responses that were “Excellent” or “Very Good”"

We begin by importing the dataset by pulling the dataframe out of the 
p8105 datasets package, filtering by topic, limiting columns, and creating 
several specific response colummns from the response. A variable for the total
of the very good and excellent columns is created.
```{r brfss_import}
brfss <- p8105.datasets::brfss_smart2010 %>% #Pulls dataframe out of package.
  janitor::clean_names() %>%  #Converts to snake case.
  filter(topic == "Overall Health") %>%  #Filters by overall health topoc.
  select(-class,-topic,-question,-sample_size,
         -(confidence_limit_low:geo_location)) %>% 
  #retains only needed columns
    spread(key = response,value=data_value) %>% 
  #converts the format to wide (resulting in the excellent and very good columns.
   janitor::clean_names() %>%
  #Cleans names again, which is necessary as new columns were created.
  mutate(proportion_ex_very_good = excellent + very_good / (excellent + fair + good + poor + very_good) ) #adds a column for the sum
#of the excellent and very good columns.
brfss %>% 
  select(proportion_ex_very_good,excellent:very_good,
         everything()) %>% arrange(-proportion_ex_very_good) %>% 
  head()  #displays the resulting tibble, sorted by those that said excellent
  #or very good.
```
The locations that responded most excellent or very good was in Utah, which
seems to have a high value for both excellent and very good. Perhaps everything
seems great there.

Instruction: "Using this dataset, do or answer the following:

How many unique locations are included in the dataset?
Is every state represented?
What state is observed the most?
In 2002, what is the median of the “Excellent” response value?
Make a histogram of “Excellent” response values in the year 2002.
Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010."
###### Number of unique locations in dataset
The number of unique locations is obtained by selecting the locationdesc variable
(containing locations more specific than state abbreviations) from the brfss dataset, filtering using
the distinct command and calculating the number of rows in the single column 
dataframe of locations.
```{r unique_locs}
unique_locations <- brfss %>%
  select( locationdesc ) %>%
  distinct() %>%
  nrow()
unique_locations
```
The states are obtained identically, with the exception that the locationabbr
column is used. If the frequency is desired, one can utilize the table command
to obtain counts of each unique locationabbr (state).
```{r states}
n_states <- brfss %>%
  select( locationabbr ) %>%
  distinct() %>%
  nrow() 
n_states
all_states <- brfss %>%
  select( locationabbr ) %>%
  distinct() %>% 
  table() %>% 
  names()
all_states
```
We'll notice that through comparison, the only state abbreviation not found in
the standard state abbrevations is the district of columbia:
```{r}
intersect(all_states,state.abb)
int_length <- intersect(all_states,state.abb) %>%  length()
```
We'll also notice that the length of the intersection between the location 
abbreviations and state abbreviations is `r int_length`. All the states are 
accounted for.

The most observed state can be obtained by sorting the table output with
counts of each state in a decreasing order and obtaining the first five of them
with the head() function. The first one is the mosts observed.
```{r most_observed_state}
most_observed_states <- brfss %>% 
  select( locationabbr ) %>% 
  table() %>% 
  sort(decreasing = T) %>% 
  head()
most_observed_states
most_observed_state <- most_observed_states %>% 
  names() %>% .[1]
most_observed_state
```
To obtain the median value of excellent responses, filtering by year for 2002
happens from the wide-format data (created earlier with spread) and the excellent
column is pulled, NA values removed, and the median value obtained from the
column vector.
```{r median_ex_2002}
median_ex_2002  <- brfss %>% 
  filter(year == 2002) %>% 
  pull(excellent) %>% 
  na.omit() %>% 
  median()
```
The number of unique locations is `r unique_locations`, achieved by selecting
distinct locations and counting the rows.  We find that all 50 states and the 
district of columbia are observed by the distinct count of `r n_states` and
observing that all the states observed in "all_states" contain only state
or DC abbreviations.
The most observed state is `r most_observed_state` with
a count of `r most_observed_states[[1]]`(as shown previously).
The median of excellent responses in 2002 was `r median_ex_2002`.
A histogram of excellent responses in 2002 is observed below.
```{r hist_excellent}
excel_hist <- brfss %>% 
  filter(year == 2002) %>% 
    drop_na(excellent) %>%   ggplot(data = .,
  aes(x = excellent)) +
    geom_histogram(alpha = 0.5, binwidth = 1, fill = "red") +
    xlab("Excellent Response Values") +
  ylab("frequency")
print(excel_hist)
```
"Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010."
Here's a scatterplot displaying the response values by county (denoted by the color variable), over time (years along the x axis), with the response values as a Y variable.
A fitting line has been added, showing that Queens has been increasing the number of excellent responses over time while New York has been clearly decreasing.
This trend is new, starting in roughly 2005-6.
```{r scatter_excellent}
#found a public ggplot theme outside of the themes package that's "excellent"
source("https://gist.githubusercontent.com/jslefche/eff85ef06b4705e6efbc/raw/736d3dc9fe71863ea62964d9132fded5e3144ad7/theme_black.R")
ggplot(data = brfss %>%
         filter(locationdesc %in%
          c("NY - Queens County","NY - New York County") &
            year >= 2002 &
            year <= 2010),
       aes(x = year,y = excellent,color = locationdesc
           )
) +
  geom_point(alpha = 0.5, size = 5) + 
  ylab("Excellent Response Values") +
  labs(color = "county") +
  theme_black() +
  geom_smooth(se = F) 

```

