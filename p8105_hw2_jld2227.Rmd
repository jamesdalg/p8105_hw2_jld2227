---
title: "HW2 p8105 James Dalgleish jld2227"
author: "James Dalgleish"
date: "September 27, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
#### Problem 1
Instructions(I'm choosing to include these to make it a bit easier to grade):
"Read and clean the data; retain line, station, name, station latitude / 
longitude, routes served, entry, vending, entrance type, and ADA compliance.
Convert the entry variable from character (YES vs NO) to a logical variable
(the ifelse or recode function may be useful)."
```{r data_import}
subway_data = read_csv("./problem1/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% 
  select(c("line","station_name","station_latitude","station_longitude",
           starts_with("route"),"entry","vending","ada")) %>% 
  mutate(entry = recode(entry,"YES" = TRUE, "NO" = FALSE))
subway_data 
```
###### Data description
Instruction: "Write a short paragraph about this dataset – 
explain briefly what variables the dataset contains, 
describe your data cleaning steps so far,
and give the dimension (rows x columns) of the resulting dataset.
Are these data tidy?"

Essentially, the dataset contains two character variables dening the subway line and station,
two numeric (double) variables denoting geolocation (latitude and longitude),
11 variables denoting the routes (mostly populated with missing values) in a wide format,
two logical TRUE/FALSE variables for entry and ada,
and a character variable with values "YES" and "NO" variable for vending.
The data is not tidy for at least the reasons that routes are in a wide format 
where the route is encoded in the column. This can be remedied with the 
gather function. As we'll see below, route1 was been encoded in
mixed case(see the "E" and "e"?),
which may mess up some downstream analysis as well.

The dataset contains `r nrow(subway_data)` rows and `r ncol(subway_data)`
columns. Data cleaning included reading the data into a tibble, 
selecting the necessary columns, converting the column names 
into a standard format, and converting the entry variable into a logical column.
```{r describe}
skimr::skim( subway_data )
subway_data %>% 
  select(starts_with( "route" )) %>%
  sapply(., table)
```
 Putting the wide format into long could aid analysis and can be done with a simple gather command. We'll also get rid of names that appear to be the same, but differ only on case. There are stations with FS and GS lines, but
which might seem like more than one line in a single observation, but they are distinct lines if one looks an MTA licensed site:
https://moveonmap.com/nyc/lnG_GS_FS/
If they were F and S incorrectly noted in the same row, we could consider using separate_rows() to fix that.
We'll use the table function to show that we've now fixed the lower and upper case e.
```{r wide_to_long}
subway_data_long = subway_data %>% 
  gather(key = "route_number", value = "train", route1:route11) %>% 
  mutate(train = tolower(train))
subway_data_long
subway_data_long %>% 
  pull( train ) %>% 
  table( )
```
####### Distinct stations
Instruction: "Answer the following questions using these data:

How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St A/B/C/D; 125st 1; 125st 4/5); the distinct function may be useful here.
How many stations are ADA compliant?
What proportion of station entrances / exits without vending allow entrance?"

We'll now select the columns of interest and limit to only the distinct rows,
then count the number of distinct rows containing route number and train
combinations.
```{r distinct_stations}
subway_routes_trains <- subway_data_long %>%
  select("route_number", "train") %>%
  distinct() %>% 
  na.omit() %>% 
  arrange(route_number,train)
subway_routes_trains
nrow(subway_routes_trains)
```
There happen to be `r nrow(subway_routes_trains)` distinct stations, according to the data.

####### Ada compliance
We will create an additional variable denoting if the station ada is compliant. There can be multiple entrances, only some of which are ada compliant. As a simple rule, a station is compliant if there is at least one station name/line combination with an ada compliant entrance, then it is ada compliant.
```{r n_ada_compliant_stations}
ada_accessible_stations <- subway_data_long %>%
  select("route_number","train","ada") %>% 
  filter( ada == TRUE) %>%
  distinct(route_number,train, .keep_all = TRUE) %>% 
  na.omit %>% 
  arrange(route_number,train)
ada_accessible_stations
```

####### Station entrances/exists that without vending which allow entry.
Instruction: "What proportion of station entrances /
exits without vending allow entrance?"
The original form "contains information related to each entrance and exit for
each subway station in NYC."
So, we will use this format to answer a question about entrances.
Careful verification through subsetting for duplicate locations reveals that 
this is the case.
Even looking at the latitude and longitude of the entrance does not distinctly 
identify the station.
In some cases, there is a different kind of entrance (elevator/escalator) or
the station is located on a different
corner of an intersection.
Converting wide to long as before could mislead someone to thinking that
there are more entrances than exist in cases where the entrance services 
multiple routes.
Therefore, the way to solve this problem is simple...
read the original data in with minimal processing, filter on the vending and
entry criteria, then count the number of rows. Using the distinct function
can prove again, that the rows are distinct.
```{r ent_exit_wo_vending}
subway_ent_exit_locations = read_csv("./problem1/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names()
subway_ent_exit_locations %>% 
  distinct()  %>% 
  nrow()
subway_ent_exit_locations %>% 
  nrow()
duplicated_locations<-subway_ent_exit_locations %>% filter(duplicated(entrance_location)) %>% 
  pull(entrance_location)
duplicated_locations_subset<-subway_ent_exit_locations %>% 
  filter(entrance_location %in% duplicated_locations) %>% 
  arrange()
enterable_novend <- subway_ent_exit_locations %>% 
  filter(.,vending == "NO" & entry == "YES") 
enterable_novend %>% 
  nrow()
enterable_novend %>% 
  distinct() %>% 
  nrow()
```

The number of distinct enterable subway entrances/exits without vending is
therefore `r enterable_novend %>% distinct() %>% nrow()`.

Instruction: "Reformat data so that route number and route name are distinct variables. 
How many distinct stations serve the A train? How many are ADA compliant?"

First we'll tackle the number of distinct A train stations. We've done most of the work by using gather() earlier.
The problem description's route name I have chosen to call train
(the value argument in gather).
I will change the name to make this clearer for the reader.
```{r distinct_a_stations}
subway_data_long_a = subway_data %>% 
  gather(key = "route_number",value = "route_name",route1:route11) %>% 
  mutate(line = tolower(line))   %>% 
  filter(route_name == "A") %>% 
  distinct(station_name,line) %>%
  arrange(station_name,line)
subway_data_long_a
```

It becomes clear now that there are precicsely `r nrow(subway_data_long_a)`
stations that service the A train.

Finally, how many are ada compliant follows a nearly identical process using the 
filter function.
```{r n_ada_stations}
subway_data_long_ada = subway_data %>% 
  gather(key = "route_number",value = "route_name",route1:route11) %>% 
  filter(ada == TRUE) %>% 
  mutate(line = tolower(line))   %>% 
  distinct(station_name,line) %>%
  arrange(station_name,line)
subway_data_long_ada
```

It becomes clear now that there are precicsely `r nrow(subway_data_long_ada)`
stations where there is an accessible entrance. Assuming one entrance is
sufficent for compliance per station, then this represents the number of
accessible stations.

#### Problem 2
instruction: "Read and clean the Mr. Trash Wheel sheet:

*specify the sheet in the Excel file and to omit columns containing notes (using the range argument and cell_cols() function)
*use reasonable variable names
*omit rows that do not include dumpster-specific data
*rounds the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)"

"Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful)."

I've chosen to read in the data, specify the sheet, columns, drop the total rows
(which typically have NA in dumpster), and removed any row without a month that
matches base-r's month names constants. This means that if the month isn't one of 
"January", "February", or any of the other ones, the row is removed.
One line has an incorrect year (the 1/2/2017 entry). This, along with the empty
yeared rows near the end of the table, as indicated by missmap, will be corrected by reassigning the year.
lubridate::year() can be used instead of format() and as.integer(),
I've chosen to use missmap to show that I've fixed the missing values correctly.
read_excel doesn't convert to a tibble, so I've chosen to do that for purposes of display.

```{r import_tidy_mr_trash_wheel}
trash_wheel_data <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
  sheet =  "Mr. Trash Wheel",
  range = readxl::cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  filter( month %in% month.name) %>% 
  mutate( year = lubridate::year(date),
    sports_balls = round(sports_balls) %>%
            as.integer()) %>% 
  as.tibble()
 trash_wheel_data
```
Instruction:  "Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful)."
```{r import_precip}
precip_2017 <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
  sheet =  "2017 Precipitation",
  skip = 2, col_names = c("month", "tot_precip")) %>% 
  na.omit() %>% 
  as.tibble() %>% 
  mutate(year = 2017)
precip_2016 <- readxl::read_excel(path = 
    "./problem2/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
  sheet =  "2016 Precipitation", 
  skip = 2,
  col_names = c("month", "tot_precip")) %>% 
  na.omit() %>% 
  as.tibble() %>% 
  mutate(year = 2016)
precip_2017_2016 <- dplyr::bind_rows(precip_2016,precip_2017) %>% 
  mutate(month = month.name[month])
avg_wt_removed <- trash_wheel_data %>%
  group_by(year) %>%
  summarise(sum=sum(weight_tons)) %>%
  pull(sum) %>% 
  mean()
homes_yr_sd <- trash_wheel_data %>%
  group_by(year) %>%
  summarise(sum=sum(homes_powered)) %>%
  pull(sum) %>%  sd()
homes_yr_mean <- trash_wheel_data %>%
  group_by(year) %>%
  summarise(sum=sum(homes_powered)) %>%
  pull(sum) %>%  mean()
```
Instruction:"Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables."

The number of observations in the trash wheel data was `r nrow(trash_wheel_data)` and `r nrow(precip_2017_2016)` observations in the precipitation data. The key variables might be the more reportable effects the intervention had on the environment, namely the average trash removed per year , `r avg_wt_removed` tons, or the postitive outcomes outside of trash removal, namely the average homes powered per year, `r homes_yr_mean` homes along with it's minimal standard deviation `r homes_yr_sd` homes.  

Instruction: " For available data, what was the total precipitation in 2017? What was the median number of sports balls in a dumpster in 2016?"
The total precipitation in 2017 was `r precip_2017 %>% pull(tot_precip) %>% sum()` inches.
The median number of sports balls was `r trash_wheel_data %>% pull(sports_balls) %>% median()`.




```{r}
sapply(X = trash_wheel_data, function(x) if( is.numeric(x) ) { e1071::kurtosis(x) } )
sapply(X = trash_wheel_data, function(x) if( is.numeric(x) ) { e1071::kurtosis(x) } )
sapply(X = trash_wheel_data, function(x) if( is.numeric(x) ) {  hist(x) } )
sapply(colnames(trash_wheel_data),function(x) if(is.numeric(trash_wheel_data[,x])) {  hist(trash_wheel_data[,x],main=x) })
#sapply(colnames(trash_wheel_data),function(x) { is.numeric(trash_wheel_data[,x])})
density(trash_wheel_data$cigarette_butts) %>% plot()
density(trash_wheel_data$grocery_bags) %>% plot()
```
# Problem 3

"This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package.

For this question:

format the data to use appropriate variable names;
focus on the “Overall Health” topic
exclude variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation
structure data so that responses (excellent to poor) are variables taking the value of Data_value
create a new variable showing the proportion of responses that were “Excellent” or “Very Good”"
```{r brfss_import}
brfss <- p8105.datasets::brfss_smart2010 %>% janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class,-topic,-question,-sample_size,-(confidence_limit_low:geo_location)) %>% 
    spread(key = response,value=data_value) %>%
   janitor::clean_names() %>%
  mutate(ex_very_good = excellent + very_good)
brfss  
```

Instruction: "Using this dataset, do or answer the following:

How many unique locations are included in the dataset?
Is every state represented?
What state is observed the most?
In 2002, what is the median of the “Excellent” response value?
Make a histogram of “Excellent” response values in the year 2002.
Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010."
###### Number of unique locations in dataset

```{r unique_locs}
unique_locations <- brfss %>%
  select( locationdesc ) %>%
  distinct() %>%
  nrow()
unique_locations
```
```{r states}
n_states <- brfss %>%
  select( locationabbr ) %>%
  distinct() %>%
  nrow() 
n_states
all_states <- brfss %>%
  select( locationabbr ) %>%
  distinct() %>% 
  table() %>% 
  names()
all_states
most_observed_states <- brfss %>% 
  select( locationabbr ) %>% 
  table() %>% 
  sort(decreasing = T) %>% 
  head()
most_observed_states
most_observed_state <- most_observed_states %>% 
  names() %>% .[1]
most_observed_state
```
```{r median_ex_2002}
median_ex_2002  <- brfss %>% 
  filter(year == 2002) %>% 
  pull(excellent) %>% 
  na.omit() %>% 
  median()
```
The number of unique locations is `r unique_locations`, achieved by selecting
distinct locations and counting the rows.  We find that all 50 states and the 
district of columbia are observed by the distinct count of `r n_states` and observing that all the states observed in "all_states" contain only state or DC abbreviations.
The most observed state is `r most_observed_state`.
The median of excellent responses in 2002 was `r median_ex_2002`.
A histogram of excellent responses in 2002 is observed before.
```{r hist_excellent}
# excellent_response_values <- brfss %>% 
#   filter(year == 2002) %>% 
#   pull(excellent) %>% 
#   na.omit() 
#   ggplot(data = NULL,
#          aes(x = excellent_response_values)) +
#     geom_histogram()
  ggplot(data = brfss %>% 
  filter(year == 2002) %>% 
    drop_na(excellent),
  aes(x=excellent)) +
    geom_histogram(binwidth = 1) +
    xlab("Excellent Response Values")
  

```
"Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010."
Here's a scatterplot displaying the response values by county (denoted by the color variable), over time (years along the x axis), with the response values as a Y variable.
A fitting line has been added, showing that Queens has been increasing the number of excellent responses over time while New York has been clearly decreasing.
This trend is new, starting in roughly 2005-6.
```{r scatter_excellent}
ggplot(data = brfss %>%
         filter(locationdesc %in%
          c("NY - Queens County","NY - New York County") &
            year >= 2002 &
            year <= 2010),
       aes(x = year,y = excellent,color = locationdesc
           )
) +
  geom_point(alpha = 0.5, size = 5) + 
  ylab("Excellent Response Values") +
  labs(color = "county") +
  theme_dark() +
  geom_smooth(se = F) 

```

